{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Distance Metrics for Probability Distribution and Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small tutorial to illustrate the new distance functions.\n",
    "\n",
    "We would need this mostly when comparing how similar two probability distributions are, and in the case of gensim, usually for LSI or LDA topic distributions after we have a LDA model.\n",
    "\n",
    "Gensim already has functionalities for this, in the sense of getting most similar documents - [this](http://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html), [this](https://radimrehurek.com/gensim/tut3.html) and [this](https://radimrehurek.com/gensim/similarities/docsim.html) are such examples of documentation and tutorials.\n",
    "\n",
    "What this tutorial shows is a building block of these larger methods, which are a small suite of distance metrics.\n",
    "We'll start by setting up a small corpus and showing off the methods.\n",
    "\n",
    "\n",
    "一个小教程来说明新的距离函数。\n",
    "\n",
    "在比较两个概率分布的相似程度时，我们最需要这样做，在gensim的情况下，通常是在我们有了LDA模型之后，针对LSI或LDA主题分布。\n",
    "\n",
    "Gensim已经具备了这方面的功能，在获取大多数类似文档的意义上——这、这和这都是文档和教程的示例。\n",
    "\n",
    "本教程展示的是这些较大方法的构建块，这些方法是一小套距离度量。我们将首先建立一个小的语料库并展示这些方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典： Dictionary(16 unique tokens: ['bank', 'river', 'shore', 'water', 'fast']...)\n",
      "语料库： [[(0, 1), (1, 1), (2, 1), (3, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (3, 1), (5, 1), (7, 1)], [(0, 2), (1, 1), (3, 1), (8, 1)], [(1, 1), (3, 1), (6, 1), (9, 1)], [(0, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (11, 1), (13, 1)], [(0, 1), (10, 1)], [(0, 1), (10, 1), (11, 1), (14, 1)], [(13, 1), (14, 1)], [(0, 1), (14, 1), (15, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# you can use any corpus, this is just illustratory\n",
    "\n",
    "texts = [['bank','river','shore','water'],\n",
    "        ['river','water','flow','fast','tree'],\n",
    "        ['bank','water','fall','flow'],\n",
    "        ['bank','bank','water','rain','river'],\n",
    "        ['river','water','mud','tree'],\n",
    "        ['money','transaction','bank','finance'],\n",
    "        ['bank','borrow','money'], \n",
    "        ['bank','finance'],\n",
    "        ['finance','money','sell','bank'],\n",
    "        ['borrow','sell'],\n",
    "        ['bank','loan','sell']]\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "print(\"字典：\",dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(\"语料库：\",corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.158*\"bank\" + 0.106*\"water\" + 0.105*\"river\" + 0.091*\"money\" + 0.068*\"tree\" + 0.068*\"sell\" + 0.065*\"finance\" + 0.046*\"flow\" + 0.045*\"borrow\" + 0.042*\"mud\"'),\n",
       " (1,\n",
       "  '0.206*\"bank\" + 0.108*\"water\" + 0.081*\"finance\" + 0.076*\"sell\" + 0.068*\"river\" + 0.066*\"borrow\" + 0.064*\"flow\" + 0.059*\"fall\" + 0.053*\"rain\" + 0.044*\"money\"')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy.random.seed(1) # setting random seed to get the same results each time.\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a few sample documents and get them ready to test Similarity. Let's call the 1st topic the water topic and the second topic the finance topic.\n",
    "\n",
    "Note: these are all distance metrics. This means that a value between 0 and 1 is returned, where values closer to 0 indicate a smaller 'distance' and therefore a larger similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_water： [(1, 1), (2, 1), (3, 1)]\n",
      "bow_finance: [(10, 1), (11, 1), (14, 1)]\n",
      "bow_bank: [(0, 1), (3, 1), (6, 1), (10, 1)]\n",
      "lda_bow_water: [(0, 0.8328832), (1, 0.16711684)]\n",
      "lda_bow_finance: [(0, 0.7771238), (1, 0.22287622)]\n",
      "lda_bow_bank： [(0, 0.7782738), (1, 0.22172624)]\n",
      "finance-water: 1.003427238824363\n",
      "finance-bank: 0.6347732788629366\n"
     ]
    }
   ],
   "source": [
    "doc_water = ['river', 'water', 'shore']\n",
    "doc_finance = ['finance', 'money', 'sell']\n",
    "doc_bank = ['finance', 'bank', 'tree', 'water']\n",
    "\n",
    "# now let's make these into a bag of words format\n",
    "##实现CBOW的方法\n",
    "bow_water = model.id2word.doc2bow(doc_water)   \n",
    "print(\"bow_water：\",bow_water)\n",
    "bow_finance = model.id2word.doc2bow(doc_finance)   \n",
    "print(\"bow_finance:\",bow_finance)\n",
    "bow_bank = model.id2word.doc2bow(doc_bank)   \n",
    "print(\"bow_bank:\",bow_bank)\n",
    "#print(\"2：\",model.id2word.doc2bow([\"bank\"]))\n",
    "# we can now get the LDA topic distributions for these\n",
    "lda_bow_water = model[bow_water]\n",
    "print(\"lda_bow_water:\",lda_bow_water)\n",
    "lda_bow_finance = model[bow_finance]\n",
    "print(\"lda_bow_finance:\",lda_bow_finance)\n",
    "lda_bow_bank = model[bow_bank]\n",
    "print(\"lda_bow_bank：\",lda_bow_bank)\n",
    "print(\"finance-water:\",((0.8548162-0.14528602)**2 + (0.14518377-0.854714)**2)**0.5)\n",
    "print(\"finance-bank:\",((0.8548162-0.40596375)**2 + (0.14518377-0.5940363)**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.8597928477777432\n",
      "2: 0.8397793894973307\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "print(\"1:\",jaccard(lda_bow_water, lda_bow_finance))\n",
    "print(\"2:\",jaccard(lda_bow_finance, lda_bow_bank))\n",
    "#print(\"3:\",jaccard(lda_bow_bank, lda_bow_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hellinger and Kullback–Leibler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to apply our distance metrics.\n",
    "\n",
    "Let's start with the popular Hellinger distance. \n",
    "The Hellinger distance metric gives an output in the range [0,1] for two probability distributions, with values closer to 0 meaning they are more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5528494030449027"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "hellinger(lda_bow_water, lda_bow_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5167124677493228"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(lda_bow_finance, lda_bow_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03903241214774338"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(lda_bow_bank, lda_bow_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense, right? In the first example, Document 1 and Document 2 are hardly similar, so we get a value of roughly 0.5. \n",
    "\n",
    "In the second case, the documents are a lot more similar, semantically. Trained with the model, they give a much less distance value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run similar examples down with Kullback Leibler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0058879964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(lda_bow_water, lda_bow_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.100847"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(lda_bow_finance, lda_bow_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE!*\n",
    "\n",
    "KL is not a Distance Metric in the mathematical sense, and hence is not symmetrical. \n",
    "This means that `kullback_leibler(lda_bow_finance, lda_bow_bank)` is not equal to  `kullback_leibler(lda_bow_bank, lda_bow_finance)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1573412"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see, the values are not equal. We'll get more into the details of this later on in the notebook.\n",
    "kullback_leibler(lda_bow_bank, lda_bow_finance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous examples we saw that there were lower distance values between bank and finance than for bank and water, even if it wasn't by a huge margin. What does this mean?\n",
    "\n",
    "The `bank` document is a combination of both water and finance related terms - but as bank in this context is likely to belong to the finance topic, the distance values are less between the finance and bank bows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.8236426), (1, 0.1763574)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to confirm our suspicion that the bank bow is more to do with finance:\n",
    "\n",
    "model.get_document_topics(bow_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident that while it isn't too skewed, it it more towards the finance topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metrics (also referred to as similarity metrics), as suggested in the examples above, are mainly for probability distributions, but the methods can accept a bunch of formats for input. You can do some further reading on [Kullback Leibler](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) and [Hellinger](https://en.wikipedia.org/wiki/Hellinger_distance) to figure out what suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the [Jaccard Distance](https://en.wikipedia.org/wiki/Jaccard_index) metric for similarity between bags of words (i.e, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_water: [(1, 1), (2, 1), (3, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8571428571428572"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"bow_water:\",bow_water)\n",
    "jaccard(bow_water, bow_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_water： ['river', 'water', 'shore']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"doc_water：\",doc_water)\n",
    "jaccard(doc_water, doc_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(['word'], ['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(['word','word1'], ['word','word2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three examples above feature 2 different input methods. \n",
    "\n",
    "In the first case, we present to jaccard document vectors already in bag of words format. The distance can be defined as 1 minus the size of the intersection upon the size of the union of the vectors. \n",
    "\n",
    "We can see (on manual inspection as well), that the distance is likely to be high - and it is. \n",
    "\n",
    "The last two examples illustrate the ability for jaccard to accept even lists (i.e, documents) as inputs.\n",
    "In the last case, because they are the same vectors, the value returned is 0 - this means the distance is 0 and they are very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics for Topic Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are already standard methods to identify similarity of documents, our distance metrics has one more interesting use-case: topic distributions. \n",
    "\n",
    "Let's say we want to find out how similar our two topics are, water and finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_finance[1]: 0.185*\"bank\" + 0.113*\"money\" + 0.103*\"sell\" + 0.093*\"finance\" + 0.068*\"borrow\" + 0.058*\"water\" + 0.056*\"transaction\" + 0.053*\"loan\" + 0.050*\"fall\" + 0.047*\"flow\"\n",
      "finance_distribution: [(0, 0.185), (11, 0.113), (14, 0.103), (10, 0.093), (13, 0.068), (3, 0.058), (12, 0.056), (15, 0.053), (7, 0.05), (5, 0.047)]\n",
      "water_distribution: [(0, 0.173), (3, 0.146), (1, 0.131), (6, 0.069), (5, 0.058), (10, 0.054), (14, 0.047), (8, 0.046), (2, 0.044), (4, 0.043)]\n"
     ]
    }
   ],
   "source": [
    "topic_water, topic_finance = model.show_topics()\n",
    "#print(\"topic_water:\",topic_water)\n",
    "# some pre processing to get the topics in a format acceptable to our distance metrics\n",
    "\n",
    "def make_topics_bow(topic):\n",
    "    # takes the string returned by model.show_topics()\n",
    "    # split on strings to get topics and the probabilities\n",
    "    topic = topic.split('+')\n",
    "    #print(\"topic:\",topic)\n",
    "    # list to store topic bows\n",
    "    topic_bow = []\n",
    "    for word in topic:\n",
    "        # split probability and word\n",
    "        prob, word = word.split('*')\n",
    "        # get rid of spaces\n",
    "        word = word.replace(\" \",\"\")\n",
    "        #print(\"word:\",word)\n",
    "        # convert to word_type\n",
    "        #print(\"2：\",model.id2word.doc2bow([\"bank\"]))\n",
    "        #print(\"example:\",model.id2word.doc2bow([word.replace('\"','')]))\n",
    "        word = model.id2word.doc2bow([word.replace('\"','')])[0][0]\n",
    "        topic_bow.append((word, float(prob)))\n",
    "    return topic_bow\n",
    "print(\"topic_finance[1]:\",topic_finance[1])\n",
    "finance_distribution = make_topics_bow(topic_finance[1])\n",
    "print(\"finance_distribution:\",finance_distribution)\n",
    "water_distribution = make_topics_bow(topic_water[1])\n",
    "print(\"water_distribution:\",water_distribution)\n",
    "\n",
    "# the finance topic in bag of words format looks like this:\n",
    "#finance_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our topics in a format more acceptable by our functions, let's use a Distance metric to see how similar the word distributions in the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957552974303146"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(water_distribution, finance_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our value of roughly 0.36 means that the topics are not TOO distant with respect to their word distributions.\n",
    "This makes sense again, because of overlapping words like `bank` and a small size dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some things to take care of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example we didn't use Kullback Leibler to test for similarity for a reason - KL is not a Distance 'Metric' in the technical sense (you can see what a metric is [here](https://en.wikipedia.org/wiki/Metric_(mathematics)). The nature of it, mathematically also means we must be a little careful before using it, because since it involves the log function, a zero can mess things up. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 16 here is the number of features the probability distribution draws from\n",
    "kullback_leibler(water_distribution, finance_distribution, 16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't very helpful, right? This just means that we have to be a bit careful about our inputs. Our old example didn't work out because they were some missing values for some words (because `show_topics()` only returned the top 10 topics). \n",
    "\n",
    "This can be remedied, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260778"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return ALL the words in the dictionary for the topic-word distribution.\n",
    "topic_water, topic_finance = model.show_topics(num_words=len(model.id2word))\n",
    "\n",
    "# do our bag of words transformation again\n",
    "finance_distribution = make_topics_bow(topic_finance[1])\n",
    "water_distribution = make_topics_bow(topic_water[1])\n",
    "\n",
    "# and voila!\n",
    "kullback_leibler(water_distribution, finance_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the distance for this is quite less, indicating a high similarity. This may be a bit off because of the small size of the corpus, where all topics are likely to contain a decent overlap of word probabilities. You will likely get a better value for a bigger corpus.\n",
    "\n",
    "So, just remember, if you intend to use KL as a metric to measure similarity or distance between two distributions, avoid zeros by returning the ENTIRE distribution. Since it's unlikely any probability distribution will ever have absolute zeros for any feature/word, returning all the values like we did will make you good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So - what exactly are Distance Metrics? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having seen the practical usages of these measures (i.e, to find similarity), let's learn a little about what exactly Distance Measures and Metrics are. \n",
    "\n",
    "I mentioned in the previous section that KL was not a distance metric. There are 4 conditons for for a distance measure to be a matric:\n",
    "\n",
    "1.\td(x,y) >= 0\n",
    "2.  d(x,y) = 0 <=> x = y\n",
    "3.  d(x,y) = d(y,x)\n",
    "4.  d(x,z) <= d(x,y) + d(y,z)\n",
    "\n",
    "That is: it must be non-negative; if x and y are the same, distance must be zero; it must be symmetric; and it must obey the triangle inequality law. \n",
    "\n",
    "Simple enough, right? \n",
    "Let's test these out for our measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25035161498501074"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal Hellinger\n",
    "hellinger(water_distribution, finance_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25035161498501074"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we swap finance and water distributions and get the same value. It is indeed symmetric!\n",
    "hellinger(finance_distribution, water_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we pass the same values, it is zero.\n",
    "hellinger(water_distribution, water_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5167124677493228"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for triangle inequality let's use LDA document distributions\n",
    "hellinger(lda_bow_finance, lda_bow_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.591881815192646"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Triangle inequality works too!\n",
    "hellinger(lda_bow_finance, lda_bow_water) + hellinger(lda_bow_water, lda_bow_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Hellinger is indeed a metric. Let's check out KL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25089684"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(finance_distribution, water_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260778"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(water_distribution, finance_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately notice that when we swap the values they aren't equal! One of the four conditions not fitting is enough for it to not be a metric. \n",
    "\n",
    "However, just because it is not a metric, (strictly in the mathematical sense) does not mean that it is not useful to figure out the distance between two probability distributions. KL Divergence is widely used for this purpose, and is probably the most 'famous' distance measure in fields like Information Theory.\n",
    "\n",
    "For a nice review of the mathematical differences between Hellinger and KL, [this](http://stats.stackexchange.com/questions/130432/differences-between-bhattacharyya-distance-and-kl-divergence) link does a very good job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings us to the end of this small tutorial.\n",
    "The scope for adding new similarity metrics is large, as there exist an even larger suite of metrics and methods to add to the matutils.py file. ([This](http://nzcsrsc08.canterbury.ac.nz/site/proceedings/Individual_Papers/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf) is one paper which talks about some of them)\n",
    "\n",
    "Looking forward to more PRs towards this functionality in Gensim! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sims: [(0, 0.29681402), (1, 0.26814798), (2, 0.6278792), (3, 0.28040192), (4, 0.28666553), (5, 0.9995542), (6, 0.9997273), (7, 0.9890071), (8, 0.9996691), (9, 0.9969551), (10, 0.99989146)]\n",
      "['bank', 'loan', 'sell'] 0.99989146\n",
      "['bank', 'borrow', 'money'] 0.9997273\n",
      "['finance', 'money', 'sell', 'bank'] 0.9996691\n",
      "['money', 'transaction', 'bank', 'finance'] 0.9995542\n",
      "['borrow', 'sell'] 0.9969551\n",
      "['bank', 'finance'] 0.9890071\n",
      "['bank', 'water', 'fall', 'flow'] 0.6278792\n",
      "['bank', 'river', 'shore', 'water'] 0.29681402\n",
      "['river', 'water', 'mud', 'tree'] 0.28666553\n",
      "['bank', 'bank', 'water', 'rain', 'river'] 0.28040192\n",
      "['river', 'water', 'flow', 'fast', 'tree'] 0.26814798\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(model[corpus])    ##创建索引\n",
    "#print(\"corpus:\",corpus)\n",
    "#print(\"index:\",index)\n",
    "#print(\"lda_bow_finance:\",lda_bow_finance)\n",
    "sims = index[lda_bow_finance]\n",
    "print(\"sims:\",list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "for doc_id, similarity in sims:\n",
    "    print(texts[doc_id], similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-97f4c75239a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a4607aafbce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0;31m　\u001b[0m\u001b[0;31m　\u001b[0m\u001b[0mBut\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0medge\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtown\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrills\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mdriven\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mhis\u001b[0m \u001b[0mmind\u001b[0m \u001b[0mby\u001b[0m \u001b[0msomething\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mAs\u001b[0m \u001b[0mhe\u001b[0m \u001b[0msat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0musual\u001b[0m \u001b[0mmorning\u001b[0m \u001b[0mtraffic\u001b[0m \u001b[0mjam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhe\u001b[0m \u001b[0mcouldn\u001b[0m\u001b[1;34m't help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mbear\u001b[0m \u001b[0mpeople\u001b[0m \u001b[0mwho\u001b[0m \u001b[0mdressed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunny\u001b[0m \u001b[0mclothes\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mthe\u001b[0m \u001b[0mgetups\u001b[0m \u001b[0myou\u001b[0m \u001b[0msaw\u001b[0m \u001b[0mon\u001b[0m \u001b[0myoung\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mHe\u001b[0m \u001b[0msupposed\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msome\u001b[0m \u001b[0mstupid\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mfashion\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHe\u001b[0m \u001b[0mdrummed\u001b[0m \u001b[0mhis\u001b[0m \u001b[0mfingers\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msteering\u001b[0m \u001b[0mwheel\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhis\u001b[0m \u001b[0meyes\u001b[0m \u001b[0mfell\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhuddle\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthese\u001b[0m \u001b[0mweirdos\u001b[0m \u001b[0mstanding\u001b[0m \u001b[0mquite\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mwhispering\u001b[0m \u001b[0mexcitedly\u001b[0m \u001b[0mtogether\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMr\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDursley\u001b[0m \u001b[0mwas\u001b[0m \u001b[0menraged\u001b[0m \u001b[0mto\u001b[0m \u001b[0msee\u001b[0m \u001b[0mthat\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcouple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthem\u001b[0m \u001b[0mweren\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0myoung\u001b[0m \u001b[0mat\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mwhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mman\u001b[0m \u001b[0mhad\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0molder\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mhe\u001b[0m \u001b[0mwas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mwearing\u001b[0m \u001b[0man\u001b[0m \u001b[0memerald\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgreen\u001b[0m \u001b[0mcloak\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnerve\u001b[0m \u001b[0mof\u001b[0m \u001b[0mhim\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mBut\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mit\u001b[0m \u001b[0mstruck\u001b[0m \u001b[0mMr\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDursley\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mprobably\u001b[0m \u001b[0msome\u001b[0m \u001b[0msilly\u001b[0m \u001b[0mstunt\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mthese\u001b[0m \u001b[0mpeople\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mobviously\u001b[0m \u001b[0mcollecting\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msomething\u001b[0m\u001b[1;33m...\u001b[0m \u001b[0myes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtraffic\u001b[0m \u001b[0mmoved\u001b[0m \u001b[0mon\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfew\u001b[0m \u001b[0mminutes\u001b[0m \u001b[0mlater\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMr\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDursley\u001b[0m \u001b[0marrived\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGrunnings\u001b[0m \u001b[0mparking\u001b[0m \u001b[0mlot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhis\u001b[0m \u001b[0mmind\u001b[0m \u001b[0mback\u001b[0m \u001b[0mon\u001b[0m \u001b[0mdrills\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"2:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n",
    "　　Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
    "　　The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.\n",
    "　　When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.\n",
    "　　None of them noticed a large, tawny owl flutter past the window.\n",
    "　　At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. \"Little tyke,\" chortled Mr. Dursley as he left the house. He got into his car and backed out of number four's drive.\n",
    "　　It was on the corner of the street that he noticed the first sign of something peculiar -a cat reading a map. For a second, Mr. Dursley didn't realize what he had seen -then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn't a map in sight. What could he have been thinking of? It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive -no, looking at the sign; cats couldn't read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he thought of nothing except a large order of drills he was hoping to get that day.\n",
    "　　But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn't help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn't bear people who dressed in funny clothes -the getups you saw on young people! He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr. Dursley was enraged to see that a couple of them weren't young at all; why, that man had to be older than he was, and wearing an emerald-green cloak! The nerve of him! But then it struck Mr. Dursley that this was probably some silly stunt -these people were obviously collecting for something... yes, that would be it. The traffic moved on and a few minutes later, Mr. Dursley arrived in the Grunnings parking lot, his mind back on drills.\n",
    "\"\"\"\n",
    "from gensim.summarization import summarize\n",
    "print(\"1:\",summarize(text))\n",
    "print (\"2:\",summarize(text, word_count=50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##关键词提取\n",
    "from gensim.summarization import keywords\n",
    "print (keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-496b138fa2ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmz_keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmz_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import mz_keywords\n",
    "mz_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
